[ConsoleHistoryPython]
History="\"C:\\Users\\gabev\\Downloads\\EmotionDetection-main\\EmotionDetection-main\\emotional-detection-main.py\" pip install dlib"
History=print("hello unreal")
History=for path in sys.path
History=for path in sys.path:
History=os.makedirs("C:\Program Files\Epic Games\UE_5.1\Engine\Content\Python")
History=os.makedirs("C:/Program Files/Epic Games/UE_5.1/Engine/Plugins/Animation/ControlRig/Content/Python")
History=os.makedirs("C:/Program Files/Epic Games/UE_5.1/Engine/Content/Python")
History=os.startfile("C:\Program Files\Epic Games\UE_5.1\Engine\Content\Python")
History=C:/Program Files/Epic Games\UE_5.1/Engine/Content/Python")
History=os.startfile("C:/Program Files/Epic Games\UE_5.1/Engine/Content/Python")
History=os.makedirs("C:/Program Files/Epic Games\UE_5.1/Engine/Content/Scripts")
History=os.makedirs("C:/Program Files/Epic Games/UE_5.1/Engine/Content/Scripts")
History=os.startfile("C:/Program Files/Epic Games/UE_5.1/Engine/Content/Scripts")
History="for path in sys.path:\r\n\tprint(path)"
History=os.startfile("C:/Program Files/Epic Games/UE_5.1/Engine/Content/Python)
History=import emotion-detection-main
History="\"C:\\Program Files\\Epic Games\\UE_5.1\\Engine\\Content\\Python\\unreal_test.py\""
History=import sys
History=sys.path.append("C:/Program Files/Epic Games/UE_5.1/Engine/Content/Python/")
History=import unreal
History=python3 "C:\Program Files\Epic Games\UE_5.1\Engine\Content\Python\unreal_test.py"
History=py "C:\Program Files\Epic Games\UE_5.1\Engine\Content\Python\unreal_test.py"
History=import os
History=import unreal_test
History=import numpy
History="import numpy as np\r\nfrom importlib import *\r\nimportlib.reload(np)"
History=import traceback
History=import argparse
History=import dlib
History=import cv2
History=import numpy as np
History=from os import listdir
History=from os.path import isfile, join
History=from pathlib import Path
History=from contextlib import contextmanager
History=from wide_resnet import WideResNet
History="import sys\r\nimport os, traceback, argparse, dlib, cv2\r\nimport numpy as np\r\nfrom os import listdir\r\nfrom os.path import isfile, join\r\nfrom pathlib import Path\r\nfrom contextlib import contextmanager\r\nfrom wide_resnet import WideResNet\r\nfrom keras.utils.data_utils import get_file\r\nfrom keras.models import load_model\r\nfrom keras_preprocessing.image import img_to_array\r\nfrom dotenv import load_dotenv"
History=os.startfile("C:/Program Files/Epic Games/UE_5.1/Engine/Content/Python")
History=import emotional-detection-main
History="\"C:\\Users\\gabev\\Downloads\\EmotionDetection-main\\EmotionDetection-main\\emotional-detection-main.py\""
History="import unreal\r\nimport h5py\r\nimport os, sys, traceback, argparse, dlib, cv2\r\nimport numpy as np\r\nfrom os import listdir\r\nfrom os.path import isfile, join\r\nfrom pathlib import Path\r\nfrom contextlib import contextmanager\r\nfrom wide_resnet import WideResNet\r\nfrom keras.utils.data_utils import get_file\r\nfrom keras.models import load_model\r\nfrom keras_preprocessing.image import img_to_array\r\nfrom dotenv import load_dotenv"
History="load_dotenv()\r\n\r\nHEADLESS = os.environ.get(\"HEADLESS\", \"True\")\r\nif HEADLESS.lower() == \"true\":\r\n    print(\"Running in headless mode\")\r\n    HEADLESS = True\r\nelse:\r\n    print(\"Running in non-headless mode\")\r\n    HEADLESS = False\r\n\r\n\r\n# import urllib\r\n# url=\'http://192.168.0.100:8080/shot.jpg\'\r\n\r\ndef draw_label(image, point, label, font=cv2.FONT_HERSHEY_SIMPLEX,\r\n               font_scale=0.8, thickness=1):\r\n    size = cv2.getTextSize(label, font, font_scale, thickness)[0]\r\n    x, y = point\r\n    cv2.rectangle(image, (x, y - size[1]), (x + size[0], y), (255, 0, 0), cv2.FILLED)\r\n    cv2.putText(image, label, point, font, font_scale, (255, 255, 255), thickness, lineType=cv2.LINE_AA)\r\n\r\n\r\ndef main():\r\n    modhash = \'fbe63257a054c1c5466cfd7bf14646d6\'\r\n    emotion_classes = {0: \'Angry\', 1: \'Fear\', 2: \'Happy\', 3: \'Neutral\', 4: \'Sad\', 5: \'Surprise\'}\r\n\r\n    # Define our model parameters\r\n    depth = 16\r\n    k = 8\r\n    weight_file = None\r\n    margin = 0.4\r\n    image_dir = None\r\n\r\n    classifier = load_model(\'emotion_little_vgg_2.h5\')\r\n    pretrained_model = \"https://github.com/yu4u/age-gender-estimation/releases/download/v0.5/weights.28-3.73.hdf5\"\r\n\r\n    # Get our weight file \r\n    if not weight_file:\r\n        weight_file = get_file(\"weights.28-3.73.hdf5\", pretrained_model, cache_subdir=\"pretrained_models\",\r\n                               file_hash=modhash, cache_dir=Path(sys.argv[0]).resolve().parent)\r\n    # load model and weights\r\n    img_size = 64\r\n    model = WideResNet(img_size, depth=depth, k=k)()\r\n    model.load_weights(weight_file)\r\n\r\n    detector = dlib.get_frontal_face_detector()\r\n\r\n    # Initialize Webcam\r\n    cap = cv2.VideoCapture(0)\r\n\r\n    while True:\r\n        ret, frame = cap.read()\r\n        # imgResp=urllib.request.urlopen(url)\r\n        # imgNp=np.array(bytearray(imgResp.read()),dtype=np.uint8)\r\n        # img=cv2.imdecode(imgNp,-1)\r\n        # frame=img\r\n\r\n        preprocessed_faces_emo = []\r\n\r\n        input_img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\r\n        img_h, img_w, _ = np.shape(input_img)\r\n        detected = detector(frame, 1)\r\n        faces = np.empty((len(detected), img_size, img_size, 3))\r\n\r\n        preprocessed_faces_emo = []\r\n        if len(detected) > 0:\r\n            for i, d in enumerate(detected):\r\n                x1, y1, x2, y2, w, h = d.left(), d.top(), d.right() + 1, d.bottom() + 1, d.width(), d.height()\r\n                xw1 = max(int(x1 - margin * w), 0)\r\n                yw1 = max(int(y1 - margin * h), 0)\r\n                xw2 = min(int(x2 + margin * w), img_w - 1)\r\n                yw2 = min(int(y2 + margin * h), img_h - 1)\r\n                cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)\r\n                # cv2.rectangle(img, (xw1, yw1), (xw2, yw2), (255, 0, 0), 2)\r\n                faces[i, :, :, :] = cv2.resize(frame[yw1:yw2 + 1, xw1:xw2 + 1, :], (img_size, img_size))\r\n                face = frame[yw1:yw2 + 1, xw1:xw2 + 1, :]\r\n                face_gray_emo = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\r\n                face_gray_emo = cv2.resize(face_gray_emo, (48, 48), interpolation=cv2.INTER_AREA)\r\n                face_gray_emo = face_gray_emo.astype(\"float\") / 255.0\r\n                face_gray_emo = img_to_array(face_gray_emo)\r\n                face_gray_emo = np.expand_dims(face_gray_emo, axis=0)\r\n                preprocessed_faces_emo.append(face_gray_emo)\r\n\r\n            # make a prediction for Age and Gender\r\n            results = model.predict(np.array(faces))\r\n            predicted_genders = results[0]\r\n            ages = np.arange(0, 101).reshape(101, 1)\r\n            predicted_ages = results[1].dot(ages).flatten()\r\n\r\n            # make a prediction for Emotion \r\n            emo_labels = []\r\n            for i, d in enumerate(detected):\r\n                preds = classifier.predict(preprocessed_faces_emo[i])[0]\r\n                emo_labels.append(emotion_classes[preds.argmax()])\r\n                if HEADLESS:\r\n                    age = int(predicted_ages[i])\r\n                    gender = \"F\" if predicted_genders[i][0] > 0.4 else \"M\"\r\n                    emotion = emo_labels[i]\r\n\r\n                    print(f\'{gender} {age}: {emotion}\')\r\n\r\n            if not HEADLESS:\r\n                for i, d in enumerate(detected):\r\n                    label = \"{}, {}, {}\".format(int(predicted_ages[i]), \"F\" if predicted_genders[i][0] > 0.4 else \"M\",\r\n                                                emo_labels[i])\r\n                    draw_label(frame, (d.left(), d.top()), label)\r\n                    print(emo_labels[i])\r\n\r\n        if not HEADLESS:\r\n            cv2.imshow(\"Emotion Detector\", frame)\r\n\r\n        if cv2.waitKey(1) == 13:  # 13 is the Enter Key\r\n            break\r\n\r\n    cap.release()\r\n\r\n    if not HEADLESS:\r\n        cv2.destroyAllWindows()\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    try:\r\n        main()\r\n    except:\r\n        print(traceback.format_exc())"
History="load_dotenv()\r\n\r\nHEADLESS = os.environ.get(\"HEADLESS\", \"True\")\r\nif HEADLESS.lower() == \"true\":\r\n    print(\"Running in headless mode\")\r\n    HEADLESS = True\r\nelse:\r\n    print(\"Running in non-headless mode\")\r\n    HEADLESS = False\r\n\r\n\r\n# import urllib\r\n# url=\'http://192.168.0.100:8080/shot.jpg\'\r\n\r\ndef draw_label(image, point, label, font=cv2.FONT_HERSHEY_SIMPLEX,\r\n               font_scale=0.8, thickness=1):\r\n    size = cv2.getTextSize(label, font, font_scale, thickness)[0]\r\n    x, y = point\r\n    cv2.rectangle(image, (x, y - size[1]), (x + size[0], y), (255, 0, 0), cv2.FILLED)\r\n    cv2.putText(image, label, point, font, font_scale, (255, 255, 255), thickness, lineType=cv2.LINE_AA)\r\n\r\n\r\ndef main():\r\n    modhash = \'fbe63257a054c1c5466cfd7bf14646d6\'\r\n    emotion_classes = {0: \'Angry\', 1: \'Fear\', 2: \'Happy\', 3: \'Neutral\', 4: \'Sad\', 5: \'Surprise\'}\r\n\r\n    # Define our model parameters\r\n    depth = 16\r\n    k = 8\r\n    weight_file = None\r\n    margin = 0.4\r\n    image_dir = None\r\n\r\n    classifier = load_model(\'C:\\\\Program Files\\\\Epic Games\\\\UE_5.1\\\\Engine\\\\Content\\\\Python\\\\emotion_little_vgg_2.h5\')\r\n    pretrained_model = \"https://github.com/yu4u/age-gender-estimation/releases/download/v0.5/weights.28-3.73.hdf5\"\r\n\r\n    # Get our weight file \r\n    if not weight_file:\r\n        weight_file = get_file(\"C:\\\\Program Files\\\\Epic Games\\\\UE_5.1\\\\Engine\\\\Content\\\\Python\\\\pretrained_models\\\\weights.28-3.73.hdf5\", pretrained_model, cache_subdir=\"C:\\\\Program Files\\\\Epic Games\\\\UE_5.1\\\\Engine\\\\Content\\\\Python\\\\pretrained_models\",\r\n                               file_hash=modhash, cache_dir=Path(sys.argv[0]).resolve().parent)\r\n    # load model and weights\r\n    img_size = 64\r\n    model = WideResNet(img_size, depth=depth, k=k)()\r\n    model.load_weights(weight_file)\r\n\r\n    detector = dlib.get_frontal_face_detector()\r\n\r\n    # Initialize Webcam\r\n    cap = cv2.VideoCapture(0)\r\n\r\n    while True:\r\n        ret, frame = cap.read()\r\n        # imgResp=urllib.request.urlopen(url)\r\n        # imgNp=np.array(bytearray(imgResp.read()),dtype=np.uint8)\r\n        # img=cv2.imdecode(imgNp,-1)\r\n        # frame=img\r\n\r\n        preprocessed_faces_emo = []\r\n\r\n        input_img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\r\n        img_h, img_w, _ = np.shape(input_img)\r\n        detected = detector(frame, 1)\r\n        faces = np.empty((len(detected), img_size, img_size, 3))\r\n\r\n        preprocessed_faces_emo = []\r\n        if len(detected) > 0:\r\n            for i, d in enumerate(detected):\r\n                x1, y1, x2, y2, w, h = d.left(), d.top(), d.right() + 1, d.bottom() + 1, d.width(), d.height()\r\n                xw1 = max(int(x1 - margin * w), 0)\r\n                yw1 = max(int(y1 - margin * h), 0)\r\n                xw2 = min(int(x2 + margin * w), img_w - 1)\r\n                yw2 = min(int(y2 + margin * h), img_h - 1)\r\n                cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)\r\n                # cv2.rectangle(img, (xw1, yw1), (xw2, yw2), (255, 0, 0), 2)\r\n                faces[i, :, :, :] = cv2.resize(frame[yw1:yw2 + 1, xw1:xw2 + 1, :], (img_size, img_size))\r\n                face = frame[yw1:yw2 + 1, xw1:xw2 + 1, :]\r\n                face_gray_emo = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\r\n                face_gray_emo = cv2.resize(face_gray_emo, (48, 48), interpolation=cv2.INTER_AREA)\r\n                face_gray_emo = face_gray_emo.astype(\"float\") / 255.0\r\n                face_gray_emo = img_to_array(face_gray_emo)\r\n                face_gray_emo = np.expand_dims(face_gray_emo, axis=0)\r\n                preprocessed_faces_emo.append(face_gray_emo)\r\n\r\n            # make a prediction for Age and Gender\r\n            results = model.predict(np.array(faces))\r\n            predicted_genders = results[0]\r\n            ages = np.arange(0, 101).reshape(101, 1)\r\n            predicted_ages = results[1].dot(ages).flatten()\r\n\r\n            # make a prediction for Emotion \r\n            emo_labels = []\r\n            for i, d in enumerate(detected):\r\n                preds = classifier.predict(preprocessed_faces_emo[i])[0]\r\n                emo_labels.append(emotion_classes[preds.argmax()])\r\n                if HEADLESS:\r\n                    age = int(predicted_ages[i])\r\n                    gender = \"F\" if predicted_genders[i][0] > 0.4 else \"M\"\r\n                    emotion = emo_labels[i]\r\n\r\n                    print(f\'{gender} {age}: {emotion}\')\r\n\r\n            if not HEADLESS:\r\n                for i, d in enumerate(detected):\r\n                    label = \"{}, {}, {}\".format(int(predicted_ages[i]), \"F\" if predicted_genders[i][0] > 0.4 else \"M\",\r\n                                                emo_labels[i])\r\n                    draw_label(frame, (d.left(), d.top()), label)\r\n                    print(emo_labels[i])\r\n\r\n        if not HEADLESS:\r\n            cv2.imshow(\"Emotion Detector\", frame)\r\n\r\n        if cv2.waitKey(1) == 13:  # 13 is the Enter Key\r\n            break\r\n\r\n    cap.release()\r\n\r\n    if not HEADLESS:\r\n        cv2.destroyAllWindows()\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    try:\r\n        main()\r\n    except:\r\n        print(traceback.format_exc())"
History=import wide_resnets
History=clear
History=cls

[ConsoleHistoryPythonREPL]
History="\"C:\\Users\\gabev\\Downloads\\EmotionDetection-main\\EmotionDetection-main\\emotional-detection-main.py\" -m pip install dlib"
History=import dlib

[ConsoleHistory]
History="\"C:\\Users\\gabev\\Downloads\\EmotionDetection-main\\EmotionDetection-main\\emotional-detection-main.py\" -m pip install dlib"

